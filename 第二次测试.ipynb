{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0b63fe8-49e1-42cd-a723-ab89bb17022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # 如果使用多个 GPU，设置以下参数\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # 确保 cudnn 的确定性，可能会降低性能\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 在代码开头设置随机数种子\n",
    "set_random_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa88cdeb-0f86-4c7b-b7b9-071da505e74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch 1/100 | D Loss: 0.6903 | G Loss: 0.7775 | IS: 1.1907 ± 0.0243\n",
      "Epoch 2/100 | D Loss: 0.7274 | G Loss: 0.7925 | IS: 1.7491 ± 0.0590\n",
      "Epoch 3/100 | D Loss: 0.6600 | G Loss: 0.9340 | IS: 2.0065 ± 0.0573\n",
      "Epoch 4/100 | D Loss: 0.6755 | G Loss: 0.9722 | IS: 2.0798 ± 0.1227\n",
      "Epoch 5/100 | D Loss: 0.6755 | G Loss: 0.9734 | IS: 2.0795 ± 0.1458\n",
      "Epoch 6/100 | D Loss: 0.6827 | G Loss: 1.1224 | IS: 2.8703 ± 0.1125\n",
      "Epoch 7/100 | D Loss: 0.6738 | G Loss: 1.1070 | IS: 2.6832 ± 0.1500\n",
      "Epoch 8/100 | D Loss: 0.6532 | G Loss: 1.0261 | IS: 2.8390 ± 0.2076\n",
      "Epoch 9/100 | D Loss: 0.6710 | G Loss: 1.1605 | IS: 3.0703 ± 0.2533\n",
      "Epoch 10/100 | D Loss: 0.6993 | G Loss: 0.9876 | IS: 3.3517 ± 0.1538\n",
      "Epoch 11/100 | D Loss: 0.6946 | G Loss: 1.1858 | IS: 3.0057 ± 0.1464\n",
      "Epoch 12/100 | D Loss: 0.6776 | G Loss: 0.9217 | IS: 3.3803 ± 0.1913\n",
      "Epoch 13/100 | D Loss: 0.6681 | G Loss: 1.1217 | IS: 3.5510 ± 0.1855\n",
      "Epoch 14/100 | D Loss: 0.6716 | G Loss: 1.0777 | IS: 3.3315 ± 0.2739\n",
      "Epoch 15/100 | D Loss: 0.6419 | G Loss: 1.1636 | IS: 3.3252 ± 0.2832\n",
      "Epoch 16/100 | D Loss: 0.6842 | G Loss: 0.9890 | IS: 3.7524 ± 0.1607\n",
      "Epoch 17/100 | D Loss: 0.6655 | G Loss: 1.1031 | IS: 3.4050 ± 0.3190\n",
      "Epoch 18/100 | D Loss: 0.6806 | G Loss: 1.0566 | IS: 3.8841 ± 0.3608\n",
      "Epoch 19/100 | D Loss: 0.6898 | G Loss: 1.1346 | IS: 3.4357 ± 0.3162\n",
      "Epoch 20/100 | D Loss: 0.6467 | G Loss: 0.9253 | IS: 3.5468 ± 0.3915\n",
      "Epoch 21/100 | LR Changed - Generator: 0.000100, Discriminator: 0.000100\n",
      "Epoch 21/100 | D Loss: 0.6578 | G Loss: 1.0459 | IS: 3.4002 ± 0.2074\n",
      "Epoch 22/100 | D Loss: 0.6147 | G Loss: 1.1707 | IS: 3.8543 ± 0.4013\n",
      "Epoch 23/100 | D Loss: 0.6423 | G Loss: 0.7881 | IS: 3.8286 ± 0.3612\n",
      "Epoch 24/100 | D Loss: 0.6632 | G Loss: 1.1023 | IS: 3.7163 ± 0.3360\n",
      "Epoch 25/100 | D Loss: 0.6427 | G Loss: 1.0337 | IS: 4.0328 ± 0.3977\n",
      "Epoch 26/100 | D Loss: 0.6626 | G Loss: 0.8682 | IS: 4.1478 ± 0.3544\n",
      "Epoch 27/100 | D Loss: 0.6680 | G Loss: 1.1995 | IS: 3.6956 ± 0.2352\n",
      "Epoch 28/100 | D Loss: 0.6623 | G Loss: 0.9763 | IS: 3.8957 ± 0.2629\n",
      "Epoch 29/100 | D Loss: 0.6664 | G Loss: 1.3365 | IS: 3.9258 ± 0.3625\n",
      "Epoch 30/100 | D Loss: 0.6242 | G Loss: 1.5191 | IS: 3.7137 ± 0.3040\n",
      "Epoch 31/100 | D Loss: 0.6664 | G Loss: 1.1669 | IS: 4.2495 ± 0.4277\n",
      "Epoch 32/100 | D Loss: 0.6758 | G Loss: 0.7899 | IS: 4.0189 ± 0.2308\n",
      "Epoch 33/100 | D Loss: 0.6410 | G Loss: 0.8956 | IS: 3.8827 ± 0.1884\n",
      "Epoch 34/100 | D Loss: 0.6623 | G Loss: 0.9291 | IS: 3.8959 ± 0.3485\n",
      "Epoch 35/100 | D Loss: 0.6578 | G Loss: 0.9317 | IS: 3.8891 ± 0.2372\n",
      "Epoch 36/100 | D Loss: 0.6276 | G Loss: 0.9457 | IS: 3.5475 ± 0.3967\n",
      "Epoch 37/100 | D Loss: 0.6904 | G Loss: 1.0655 | IS: 4.1919 ± 0.3166\n",
      "Epoch 38/100 | D Loss: 0.6769 | G Loss: 1.0962 | IS: 3.6030 ± 0.3288\n",
      "Epoch 39/100 | D Loss: 0.6593 | G Loss: 0.8848 | IS: 4.0902 ± 0.3304\n",
      "Epoch 40/100 | LR Changed - Generator: 0.000050, Discriminator: 0.000050\n",
      "Epoch 40/100 | D Loss: 0.6380 | G Loss: 0.9124 | IS: 3.9181 ± 0.4024\n",
      "Epoch 41/100 | D Loss: 0.6470 | G Loss: 1.0400 | IS: 3.6157 ± 0.3361\n",
      "Epoch 42/100 | D Loss: 0.6260 | G Loss: 0.8321 | IS: 3.8662 ± 0.3072\n",
      "Epoch 43/100 | D Loss: 0.6141 | G Loss: 1.1562 | IS: 3.9615 ± 0.1564\n",
      "Epoch 44/100 | D Loss: 0.6146 | G Loss: 1.2758 | IS: 3.9869 ± 0.2715\n",
      "Epoch 45/100 | D Loss: 0.5934 | G Loss: 0.9282 | IS: 3.8191 ± 0.3258\n",
      "Epoch 46/100 | D Loss: 0.6214 | G Loss: 1.1966 | IS: 4.1935 ± 0.3949\n",
      "Epoch 47/100 | D Loss: 0.6413 | G Loss: 0.8455 | IS: 3.8980 ± 0.2491\n",
      "Epoch 48/100 | D Loss: 0.6856 | G Loss: 0.7714 | IS: 4.1169 ± 0.2121\n",
      "Epoch 49/100 | D Loss: 0.5955 | G Loss: 0.9355 | IS: 3.9570 ± 0.3138\n",
      "Epoch 50/100 | D Loss: 0.6003 | G Loss: 1.0444 | IS: 3.9662 ± 0.3033\n",
      "Epoch 51/100 | D Loss: 0.6177 | G Loss: 0.9415 | IS: 3.9466 ± 0.3577\n",
      "Epoch 52/100 | D Loss: 0.6627 | G Loss: 0.7925 | IS: 4.1074 ± 0.2128\n",
      "Epoch 53/100 | D Loss: 0.6276 | G Loss: 1.3879 | IS: 3.8521 ± 0.2627\n",
      "Epoch 54/100 | D Loss: 0.6222 | G Loss: 1.2266 | IS: 3.8525 ± 0.1984\n",
      "Epoch 55/100 | D Loss: 0.6077 | G Loss: 1.2032 | IS: 3.8757 ± 0.1882\n",
      "Epoch 56/100 | D Loss: 0.6173 | G Loss: 0.8890 | IS: 4.0930 ± 0.2208\n",
      "Epoch 57/100 | D Loss: 0.6159 | G Loss: 0.8409 | IS: 4.0193 ± 0.2332\n",
      "Epoch 58/100 | D Loss: 0.5968 | G Loss: 1.1949 | IS: 3.9392 ± 0.2143\n",
      "Epoch 59/100 | D Loss: 0.6186 | G Loss: 0.8558 | IS: 4.1628 ± 0.4297\n",
      "Epoch 60/100 | D Loss: 0.6262 | G Loss: 1.3753 | IS: 4.1828 ± 0.1736\n",
      "Epoch 61/100 | D Loss: 0.6319 | G Loss: 0.9812 | IS: 4.1804 ± 0.3356\n",
      "Epoch 62/100 | D Loss: 0.6327 | G Loss: 1.4384 | IS: 3.8569 ± 0.2692\n",
      "Epoch 63/100 | D Loss: 0.6203 | G Loss: 0.7946 | IS: 4.1541 ± 0.3213\n",
      "Epoch 64/100 | D Loss: 0.6047 | G Loss: 1.2076 | IS: 4.2223 ± 0.2638\n",
      "Epoch 65/100 | D Loss: 0.6557 | G Loss: 1.3475 | IS: 3.9346 ± 0.2910\n",
      "Epoch 66/100 | D Loss: 0.6285 | G Loss: 0.9962 | IS: 3.9810 ± 0.2698\n",
      "Epoch 67/100 | D Loss: 0.6402 | G Loss: 1.3341 | IS: 4.1734 ± 0.2589\n",
      "Epoch 68/100 | D Loss: 0.6095 | G Loss: 1.0918 | IS: 4.2279 ± 0.3062\n",
      "Epoch 69/100 | D Loss: 0.5880 | G Loss: 0.8161 | IS: 4.0275 ± 0.2539\n",
      "Epoch 70/100 | LR Changed - Generator: 0.000025, Discriminator: 0.000025\n",
      "Epoch 70/100 | D Loss: 0.6445 | G Loss: 1.2305 | IS: 4.2880 ± 0.2960\n",
      "Epoch 71/100 | D Loss: 0.5957 | G Loss: 1.1437 | IS: 4.0831 ± 0.1965\n",
      "Epoch 72/100 | D Loss: 0.5880 | G Loss: 0.8533 | IS: 4.1196 ± 0.3874\n",
      "Epoch 73/100 | D Loss: 0.5896 | G Loss: 0.9164 | IS: 4.2097 ± 0.3443\n",
      "Epoch 74/100 | D Loss: 0.6305 | G Loss: 0.7800 | IS: 3.9258 ± 0.2360\n",
      "Epoch 75/100 | D Loss: 0.6222 | G Loss: 1.1602 | IS: 4.1567 ± 0.3593\n",
      "Epoch 76/100 | D Loss: 0.6319 | G Loss: 1.2496 | IS: 4.0475 ± 0.6102\n",
      "Epoch 77/100 | D Loss: 0.5867 | G Loss: 0.8038 | IS: 4.0025 ± 0.3000\n",
      "Epoch 78/100 | D Loss: 0.5885 | G Loss: 1.2769 | IS: 4.3078 ± 0.4123\n",
      "Epoch 79/100 | D Loss: 0.5923 | G Loss: 0.8915 | IS: 4.1968 ± 0.4545\n",
      "Epoch 80/100 | D Loss: 0.5777 | G Loss: 1.3413 | IS: 3.9381 ± 0.2389\n",
      "Epoch 81/100 | D Loss: 0.5942 | G Loss: 1.3368 | IS: 4.0780 ± 0.2349\n",
      "Epoch 82/100 | D Loss: 0.6023 | G Loss: 0.8402 | IS: 3.9670 ± 0.2175\n",
      "Epoch 83/100 | D Loss: 0.5791 | G Loss: 1.3945 | IS: 3.9562 ± 0.3671\n",
      "Epoch 84/100 | D Loss: 0.5526 | G Loss: 1.3599 | IS: 4.0795 ± 0.3299\n",
      "Epoch 85/100 | D Loss: 0.5739 | G Loss: 1.3732 | IS: 4.1642 ± 0.2454\n",
      "Epoch 86/100 | D Loss: 0.5520 | G Loss: 0.9815 | IS: 4.2147 ± 0.3961\n",
      "Epoch 87/100 | D Loss: 0.5855 | G Loss: 1.2404 | IS: 3.9501 ± 0.3016\n",
      "Epoch 88/100 | D Loss: 0.5894 | G Loss: 0.8621 | IS: 4.4049 ± 0.3470\n",
      "Epoch 89/100 | D Loss: 0.5988 | G Loss: 0.9456 | IS: 4.0692 ± 0.3750\n",
      "Epoch 90/100 | D Loss: 0.5059 | G Loss: 1.3437 | IS: 3.8855 ± 0.5312\n",
      "Epoch 91/100 | D Loss: 0.5863 | G Loss: 0.8722 | IS: 4.3158 ± 0.4364\n",
      "Epoch 92/100 | D Loss: 0.5572 | G Loss: 1.0463 | IS: 4.1416 ± 0.2663\n",
      "Epoch 93/100 | D Loss: 0.5611 | G Loss: 0.9544 | IS: 4.2887 ± 0.2329\n",
      "Epoch 94/100 | D Loss: 0.5794 | G Loss: 1.3943 | IS: 3.9348 ± 0.3478\n",
      "Epoch 95/100 | D Loss: 0.5631 | G Loss: 0.9445 | IS: 3.9598 ± 0.3315\n",
      "Epoch 96/100 | D Loss: 0.5940 | G Loss: 0.9069 | IS: 4.1318 ± 0.3152\n",
      "Epoch 97/100 | D Loss: 0.5726 | G Loss: 0.9532 | IS: 4.1638 ± 0.2665\n",
      "Epoch 98/100 | D Loss: 0.5965 | G Loss: 1.4643 | IS: 3.8896 ± 0.5707\n",
      "Epoch 99/100 | D Loss: 0.6084 | G Loss: 0.8675 | IS: 4.2132 ± 0.4835\n",
      "Epoch 100/100 | D Loss: 0.5545 | G Loss: 1.3110 | IS: 4.1820 ± 0.2140\n",
      "Best model saved from epoch: 88 with Inception Score: 4.4049\n",
      "Total training time: 4h 24m 36s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.models import inception_v3\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn.utils.spectral_norm as spectral_norm  # 导入谱归一化\n",
    "\n",
    "# 超参数\n",
    "latent_dim = 128\n",
    "num_classes = 100  # CIFAR-100 类别数\n",
    "img_size = 32\n",
    "channels = 3\n",
    "batch_size = 128\n",
    "n_epochs = 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # 随机水平翻转\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*channels, [0.5]*channels),\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = datasets.CIFAR100(root='data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# 自注意力层\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channel_in = in_dim\n",
    "\n",
    "        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
    "        self.key_conv   = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim,      kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax  = nn.Softmax(dim=-1)\n",
    "    def forward(self, x):\n",
    "        m_batchsize, C, width, height = x.size()\n",
    "\n",
    "        proj_query = self.query_conv(x).view(m_batchsize, -1, width * height).permute(0, 2, 1)\n",
    "        proj_key   = self.key_conv(x).view(m_batchsize, -1, width * height)\n",
    "        energy     = torch.bmm(proj_query, proj_key)\n",
    "        attention  = self.softmax(energy)\n",
    "\n",
    "        proj_value = self.value_conv(x).view(m_batchsize, -1, width * height)\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(m_batchsize, C, width, height)\n",
    "\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "# 条件批归一化\n",
    "class ConditionalBatchNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(ConditionalBatchNorm2d, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(num_features, affine=False)\n",
    "        self.embed = nn.Embedding(num_classes, num_features * 2)\n",
    "        # 初始化gamma为1，beta为0\n",
    "        self.embed.weight.data[:, :num_features].fill_(1)\n",
    "        self.embed.weight.data[:, num_features:].zero_()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = self.bn(x)\n",
    "        gamma, beta = self.embed(y).chunk(2, 1)\n",
    "        gamma = gamma.unsqueeze(2).unsqueeze(3)\n",
    "        beta  = beta.unsqueeze(2).unsqueeze(3)\n",
    "        out = gamma * out + beta\n",
    "        return out\n",
    "\n",
    "# 生成器的残差块\n",
    "class ResBlockG(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_classes):\n",
    "        super(ResBlockG, self).__init__()\n",
    "        self.bn1 = ConditionalBatchNorm2d(in_channels, num_classes)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1)\n",
    "        self.bn2 = ConditionalBatchNorm2d(out_channels, num_classes)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels or True:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        out = self.bn1(x, labels)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out, labels)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv2(out)\n",
    "        shortcut = self.shortcut(x)\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "# 判别器的残差块\n",
    "class ResBlockD(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResBlockD, self).__init__()\n",
    "        self.conv1 = spectral_norm(nn.Conv2d(in_channels, out_channels, 3, 1, 1))\n",
    "        self.activation = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.conv2 = spectral_norm(nn.Conv2d(out_channels, out_channels, 4, 2, 1))\n",
    "\n",
    "        self.shortcut = spectral_norm(nn.Conv2d(in_channels, out_channels, 4, 2, 1)) \\\n",
    "            if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv2(out)\n",
    "        shortcut = self.shortcut(x)\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "# 交叉注意力机制（Cross Attention）\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        # 将图像特征映射到Q\n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        # 将类嵌入映射到K和V\n",
    "        self.key_linear = nn.Linear(in_dim, in_dim // 8)\n",
    "        self.value_linear = nn.Linear(in_dim, in_dim)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, class_emb):\n",
    "        # x: B,C,H,W\n",
    "        # class_emb: B,C（C与x的C相同，以便融合）\n",
    "        B, C, H, W = x.size()\n",
    "        Q = self.query_conv(x).view(B, -1, H*W).permute(0,2,1) # B,HW,C'\n",
    "        K = self.key_linear(class_emb).unsqueeze(1) # B,1,C'\n",
    "        # 计算注意力权重\n",
    "        energy = torch.bmm(Q, K.transpose(1,2)) # B,HW,1\n",
    "        attention = self.softmax(energy)\n",
    "        V = self.value_linear(class_emb).unsqueeze(1) # B,1,C\n",
    "        out = torch.bmm(attention, V) # B,HW,C\n",
    "        out = out.permute(0,2,1).view(B,C,H,W)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "# 生成器\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes, channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        img_size = 32  # 固定图像尺寸为32\n",
    "\n",
    "        self.init_size = img_size // 16  # init_size = 2\n",
    "        self.l1 = nn.Linear(latent_dim, 512 * self.init_size ** 2)\n",
    "\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResBlockG(512, 512, num_classes),\n",
    "            ResBlockG(512, 256, num_classes),\n",
    "            ResBlockG(256, 128, num_classes),\n",
    "            ResBlockG(128, 64, num_classes)\n",
    "        ])\n",
    "\n",
    "        self.attention = SelfAttention(64)\n",
    "\n",
    "        # 为交叉注意力准备的类嵌入（与64通道匹配）\n",
    "        self.class_embed_ca = nn.Embedding(num_classes, 64)\n",
    "        self.cross_attention = CrossAttention(64)\n",
    "\n",
    "        # 融合特征的卷积层（将中间层特征进行上采样后与最终特征融合）\n",
    "        self.fusion_conv = nn.Conv2d(256+64, 64, 1, 1, 0)\n",
    "\n",
    "        self.bn = ConditionalBatchNorm2d(64, num_classes)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        self.conv_out = nn.Conv2d(64, channels, 3, 1, 1)\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        out = self.l1(noise)\n",
    "        out = out.view(out.size(0), 512, self.init_size, self.init_size)\n",
    "\n",
    "        # 多级特征生成\n",
    "        out1 = self.res_blocks[0](out, labels)   # 512通道, 尺寸4x4\n",
    "        out2 = self.res_blocks[1](out1, labels)  # 256通道, 尺寸8x8\n",
    "        out3 = self.res_blocks[2](out2, labels)  # 128通道, 尺寸16x16\n",
    "        out4 = self.res_blocks[3](out3, labels)  # 64通道, 尺寸32x32\n",
    "        out = out4\n",
    "\n",
    "        # 自注意力\n",
    "        out = self.attention(out)\n",
    "\n",
    "        # 交叉注意力融合类特征\n",
    "        class_emb = self.class_embed_ca(labels)  # B,64\n",
    "        out = self.cross_attention(out, class_emb)\n",
    "\n",
    "        # 特征融合（将out2上采样到out相同大小，然后拼接）\n",
    "        # out2是8x8, out是32x32\n",
    "        out2_upsampled = F.interpolate(out2, size=out.shape[2:], mode='nearest') \n",
    "        fused = torch.cat([out2_upsampled, out], dim=1) # B,(256+64),32,32\n",
    "        out = self.fusion_conv(fused) # B,64,32,32\n",
    "\n",
    "        out = self.bn(out, labels)\n",
    "        out = self.activation(out)\n",
    "        img = torch.tanh(self.conv_out(out))\n",
    "        return img\n",
    "\n",
    "# 判别器\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes, channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.initial = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(channels, 64, 3, 1, 1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResBlockD(64, 128),\n",
    "            ResBlockD(128, 256),\n",
    "            ResBlockD(256, 512)\n",
    "        ])\n",
    "\n",
    "        self.attention = SelfAttention(512)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = spectral_norm(nn.Linear(512, 1))\n",
    "        self.embed = nn.Embedding(num_classes, 512)\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        out = self.initial(img)\n",
    "        for res_block in self.res_blocks:\n",
    "            out = res_block(out)\n",
    "        out = self.attention(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        validity = self.fc(out)\n",
    "        embed = self.embed(labels)\n",
    "        prod = torch.sum(out * embed, dim=1, keepdim=True)\n",
    "        return validity + prod\n",
    "\n",
    "# 初始化模型\n",
    "generator = Generator(latent_dim, num_classes, channels).to(device)\n",
    "discriminator = Discriminator(num_classes, channels).to(device)\n",
    "\n",
    "# 损失函数和优化器\n",
    "adversarial_loss = nn.BCEWithLogitsLoss()\n",
    "# 优化器\n",
    "optimizer_G = optim.RMSprop(generator.parameters(), lr=0.0002, weight_decay=1e-3)\n",
    "optimizer_D = optim.RMSprop(discriminator.parameters(), lr=0.0002, weight_decay=1e-3)\n",
    "\n",
    "# MultiStepLR 定义调度器\n",
    "milestones = [21,40,70]  # 在这些周期降低学习率\n",
    "gamma = 0.5  # 每次降低到原来的 50%\n",
    "scheduler_G = optim.lr_scheduler.MultiStepLR(optimizer_G, milestones=milestones, gamma=gamma)\n",
    "scheduler_D = optim.lr_scheduler.MultiStepLR(optimizer_D, milestones=milestones, gamma=gamma)\n",
    "\n",
    "# 定义用于FID计算的文件夹路径\n",
    "fid_real_images_path = 'fid_images/realbiggan_images'  # 保存真实图片的路径\n",
    "fid_generated_images_path = 'fid_images/generatedbiggan_images'  # 保存生成图片的路径\n",
    "model_save_path = 'two_model.pth'  # 保存最佳模型的路径\n",
    "\n",
    "# 删除并重新创建文件夹，以清空旧图片\n",
    "for path in [fid_real_images_path, fid_generated_images_path]:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# 加载预训练的 InceptionV3 模型\n",
    "inception_model = inception_v3(weights=\"DEFAULT\").to(device)\n",
    "inception_model.eval()\n",
    "\n",
    "# 定义Inception Score计算函数\n",
    "def calculate_inception_score(images, batch_size=32, splits=10):\n",
    "    images = F.interpolate(images, size=(299, 299), mode=\"bilinear\", align_corners=False)\n",
    "    preds = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch = images[i:i + batch_size].to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = inception_model(batch)\n",
    "            preds.append(F.softmax(pred, dim=1).cpu().numpy())\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "\n",
    "    split_scores = []\n",
    "    for k in range(splits):\n",
    "        part = preds[k * (len(preds) // splits): (k + 1) * (len(preds) // splits)]\n",
    "        py = np.mean(part, axis=0)\n",
    "        scores = part * (np.log(part + 1e-6) - np.log(py + 1e-6))\n",
    "        split_scores.append(np.exp(np.mean(np.sum(scores, axis=1))))\n",
    "        \n",
    "    return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "# 初始化最佳 Inception Score 和对应的轮次\n",
    "best_is = 0\n",
    "best_epoch = 0\n",
    "\n",
    "# 训练循环\n",
    "start_time = time.time()\n",
    "previous_lr_G = optimizer_G.param_groups[0]['lr']\n",
    "previous_lr_D = optimizer_D.param_groups[0]['lr']\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(train_loader):\n",
    "        batch_size_current = imgs.size(0)\n",
    "        valid = torch.ones(batch_size_current, 1, device=device)\n",
    "        fake = torch.zeros(batch_size_current, 1, device=device)\n",
    "\n",
    "        real_imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # ---------------------\n",
    "        #  训练判别器\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # 判别真实图像\n",
    "        real_pred = discriminator(real_imgs, labels)\n",
    "        d_real_loss = adversarial_loss(real_pred, valid)\n",
    "\n",
    "        # 判别生成图像\n",
    "        z = torch.randn(batch_size_current, latent_dim, device=device)\n",
    "        gen_labels = torch.randint(0, num_classes, (batch_size_current,), device=device)\n",
    "        fake_imgs = generator(z, gen_labels)\n",
    "        fake_pred = discriminator(fake_imgs.detach(), gen_labels)\n",
    "        d_fake_loss = adversarial_loss(fake_pred, fake)\n",
    "\n",
    "        # 判别器总损失\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  训练生成器\n",
    "        # ---------------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        validity = discriminator(fake_imgs, gen_labels)\n",
    "        g_loss = adversarial_loss(validity, valid)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "    # 更新学习率调度器\n",
    "    scheduler_G.step()\n",
    "    scheduler_D.step()\n",
    "\n",
    "    current_lr_G = scheduler_G.get_last_lr()[0]\n",
    "    current_lr_D = scheduler_D.get_last_lr()[0]\n",
    "    if current_lr_G != previous_lr_G or current_lr_D != previous_lr_D:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} | LR Changed - Generator: {current_lr_G:.6f}, Discriminator: {current_lr_D:.6f}\")\n",
    "        previous_lr_G = current_lr_G\n",
    "        previous_lr_D = current_lr_D\n",
    "\n",
    "    # 保存用于FID计算的真实图片\n",
    "    with torch.no_grad():\n",
    "        real_imgs_count = len(real_imgs)\n",
    "        for idx in range(real_imgs_count):\n",
    "            real_save_path = os.path.join(fid_real_images_path, f\"real_img_epoch{epoch+1}_{idx}.png\")\n",
    "            vutils.save_image(real_imgs[idx], real_save_path, normalize=True)\n",
    "\n",
    "    # 保存生成图片用于FID计算\n",
    "    with torch.no_grad():\n",
    "        for idx in range(real_imgs_count):\n",
    "            gen_save_path = os.path.join(fid_generated_images_path, f\"gen_img_epoch{epoch+1}_{idx}.png\")\n",
    "            vutils.save_image(fake_imgs[idx], gen_save_path, normalize=True)\n",
    "\n",
    "    # 计算当前 Inception Score\n",
    "    with torch.no_grad():\n",
    "        num_eval_samples = 1000\n",
    "        eval_batch_size = 100\n",
    "        eval_imgs = []\n",
    "        for _ in range(num_eval_samples // eval_batch_size):\n",
    "            z = torch.randn(eval_batch_size, latent_dim, device=device)\n",
    "            gen_labels = torch.randint(0, num_classes, (eval_batch_size,), device=device)\n",
    "            eval_batch_imgs = generator(z, gen_labels)\n",
    "            eval_imgs.append(eval_batch_imgs.cpu())\n",
    "        eval_imgs = torch.cat(eval_imgs, dim=0)\n",
    "        mean_is, std_is = calculate_inception_score(eval_imgs, batch_size=eval_batch_size, splits=10)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f} | IS: {mean_is:.4f} ± {std_is:.4f}\")\n",
    "\n",
    "    # 如果当前 Inception Score 最佳，则保存模型\n",
    "    if mean_is > best_is:\n",
    "        best_is = mean_is\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save({\n",
    "            'epoch': best_epoch,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "            'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "            'best_is': best_is,\n",
    "        }, model_save_path)\n",
    "print(f\"Best model saved from epoch: {best_epoch} with Inception Score: {best_is:.4f}\")\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "hours = int(total_time // 3600)\n",
    "minutes = int((total_time % 3600) // 60)\n",
    "seconds = int(total_time % 60)\n",
    "print(f\"Total training time: {hours}h {minutes}m {seconds}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f613a-8ad1-44d0-8517-24cd8f4ff54f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
